How to use this tool: PLEASE READ EVERYTHING IN THIS HOW-TO BEFORE YOU START USING THIS TOOL

1. Clone this Git Repo and do the following, cd into the cloned Repo:
	a. Create an AWS user in your target AWS account i.e. where you want the data to be transferred.
	b. Create the access keys for the abovementioned user and populate it in the aws.creds file in repo's top level directory.
	c. Your file should look something like this:

		{
        		"aws_access_key_id": aws access key id in double quotes>,
        		"aws_secret_access_key": <aws secret key in double quotes>
		}

	d. Make sure you have your GCP credentilas in some file somewhere in this form,

		{
  			"type": <type of service account>,
  			"project_id": <project id>,
  			"private_key_id": <private key id>,
  			"private_key": <private key>,
  			"client_email": <client email for service account>,
  			"client_id": <client id>,
  			"auth_uri": <self explanatory>,
  			"token_uri": <self explanatory>,
  			"auth_provider_x509_cert_url": <this is provided by GCP>,
  			"client_x509_cert_url": <this is provided by GCP>
		}

		This link provide guidance on how to get these: https://cloud.google.com/iam/docs/creating-managing-service-account-keys
	d. Make sure that the build.sh file has execute permissions i.e. run the following command,

		chmod +x build.sh

2. Run the build.sh file in the directory you find it in. Like so, no arguments.

	./build.sh

3. Next, you run the packagecreds.sh script. Once this is done, you will have a full functional codebundle that you can deploy on a
Google Dataproc cluster.

	./packagecreds.sh -g <Full path to the file which has the GCP credentials as shown above in point 1.d.>

	The packagecreds.sh script expects a file named aws.creds that contains the AWS credentials in the form mentioned in
	point 1.c. above. So make sure you have that in place already i.e. before you run the packagecreds.sh.

4. Now we run the orchestrator.py script. This has all the code you need to launch the dataproc cluster, once you are done
running this script, you are ready to run your jobs that will initiate the data migration from Google BigQuery to Amazon S3.
To run the orchestrator.py, 
	
	./orchestrator.py --bucket <a GCS bucket where we store our code etc...> --prefix <a folder under which the codebundle.zip may be stored> --location <location of the cluster, defaults to US> --ziplocation <directory path to the codebundle - only the directory, the codebundle has to be always named codebundle.zip>  --initfilename <name of the initialization script> --clustername <name for your cluster> --clusterstgbucket <can be an existing staging GCS bucket> --clustertempbucket <temporary GCS bucket for the cluster - can be an existing bucket> --projectid <project id>

	here is an example of such a command:
	
	./orchestrator.py --bucket rns_my_bucket_test --prefix mycode/ --location "US" --ziplocation ./build  --initfilename cluster_initialization.sh --clustername rns-my-test-cluster --clusterstgbucket rns_my_staging_bucket --clustertempbucket rns_my_temp_bucket --projectid innate-entry-286804

5. Once the orchestrator completes successfully, you can start jobs on the cluster. To do this, where you initiate the jobs from, you
need to have gcloud installed, this would be the first job you run on the cluster,

	gcloud dataproc jobs submit pyspark file:///work/move_tables.py --cluster=rns-my-test-cluster --region=us-east1 --properties spark.jars.packages=org.apache.spark:spark-avro_2.12:2.4.7

	Though we would have liked to parameterize everything, we were constrained for time and there are certain things that you can do to
	change the behavior of the pyspark job.

	You should open the move_tables.py script and change the way it behaves by making changes to the following constants, below are
	example values for these constants. The changes have to be made prior to running the build.sh script.
		
		BQMETADATADESTINATION="rns-lk"
		GCSRAWDATABUCKET="rns_sample_data_bucket_1"
		GCSRAWDATAPREFIX="exported_tables/"
		BQRAWDATAFILESLOCATION="gs://rns_sample_data_bucket_1/exported_tables/"
		BQMIMICEDDATABUCKET="rns_sample_data_bucket_1"
		BQMIMICEDDATAPREFIX="BQSTRUCT/"

	What do each of these do?
		BQMETADATADESTINATION -> name of the Amazon S3 bucket where we intend to store the metadata, this will be used for verification/validation later
		GCSRAWDATABUCKET -> the GCS bucket where we intend to keep the raw data
		GCSRAWDATAPREFIX -> the folder under which we will populate the raw data, this will be appended with "raw/"
		BQRAWDATAFILESLOCATION -> in some cases we need the full path, and we were too lazy to fabricate it from inputs
		BQMIMICEDDATABUCKET -> the bucket where we mimic the BigQuery data organizations, in terms of partitions we mean.
		BQMIMICEDDATAPREFIX -> the prefix under which you will find the final BigQuery exported data in either parquet or JSON.

6. Once you have run the above job, we are ready for the actual data transfer, this is, at present done using a MapReduce job i.e. using
distcp, here is the command,

	gcloud dataproc jobs submit hadoop --cluster=rns-my-test-cluster --region=us-east1 --class=org.apache.hadoop.tools.DistCp -- gs://rns_sample_data_bucket_1/BQSTRUCT/ s3a://<aws access key id>:<aws secret key>@rns-bq-migration/Migrated-Data/

	In the above command, you will have to change --region, --cluster and source and destination according to your preferences.
7. Once all the data is transferred to S3, we will start creating the objects needed on the AWS side.
8. First, create an IAM role that can be used by a Glue crawler to crawl the data transferred to S3 and create the databases and tables. The role should have appropriate permissions(List, get) on the S3 bucket and permissions on the Glue data catalog to create databases and tables and has a trust relationship with Glue. Some guidance on this is provided in the AWS documentation at https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html however, it is advisable to provide minimal access to the role
Next, create a Glue python-shell job with 0.0625 DPUs using the code bq_migration.py. Inside the code, change the values for the bucket, prefix, schemafilepath(set this to the location in the S3 bucket where the BQ metadata is transferred) and the rolename(to the rolename that you created above). Run the job through the console. The job sets up a crawler to crawl the data transferred from BQ and creates the table and database definitions in the AWS Glue data catalog. It then parses the metadata for each table in the Glue catalog and creates a csv file with details of every table in the catalog. Finally, it runs a crawler on the metadata transferred by BQ and creates the required metadata tables both for the AWS and GCP sides which will be used in the subsequent steps.
Next, reate a Glue python-shell job with 0.0625 DPUs using the code validatedata.py. Inside the code, change the values for the bucket and schemafilepath(set this to the location in the S3 bucket where the BQ metadata is transferred). Run the job. The job validates the data by running queries on the S3 data using Athena and compares it with the validation data shared from the BQ side. If the values are different, it writes the data to a validation_discrepancies file which is subsequently crawled to create a validation_discrepancies table.
Next, run the queries provided in Athena_Queries.txt in Athena. These queries structurally compare the metadata from the BQ side with the metadata generated by the Glue crawlers and highlight any discrepancies.
Once all this processing is done, query the following tables to look for any discrepancies in the migration process
1. table_discrepancies - Any table name discrepancies
2. column_name_discrepancies - For any column name discrepancies - Please note that this table will pick up date based partition columns as discrepancies as they are not shared as part of the BQ metadata files 
3. column_type_discrepancies - Any discrepancies in data types
4. Validation_Discrepancies -  Any discrepancies in sum or distinct count for validation data

Any row in any of the tables shows a potential discrepancy and needs to be looked into and resolved.