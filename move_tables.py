#!/usr/bin/env python

## The imports
import sys
import os
import json
import boto3
from google.cloud import storage
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
sys.path.insert(1, '/work/BQDetailsObject')
sys.path.insert(1,'/work/BQDataExtractor')
from BQDetailsObject import BQDetailsObject
from BQDataExtractor import BQDataExtractor

## The constants
BQMETADATADESTINATION="bqmigration"
GCSRAWDATABUCKET="rns_sample_data_bucket_1"
GCSRAWDATAPREFIX="exported_tables/"
BQRAWDATAFILESLOCATION="gs://rns_sample_data_bucket_1/exported_tables/"
BQMIMICEDDATABUCKET="rns_sample_data_bucket_1"
BQMIMICEDDATAPREFIX="BQSTRUCT/"

## Set up the Working Directory
os.chdir('/work')

## Set up the environment variables
os.environ["GOOGLE_APPLICATION_CREDENTIALS"]="/work/creds/gcp.creds"

bqd = BQDetailsObject()
bqd.getTableStats()

## Setup temporary credentials with a 12 hr lifetime
with open('/work/creds/aws.creds','r') as fcreds:
    creds = json.load(fcreds)

stscli = boto3.client('sts',aws_access_key_id=creds['aws_access_key_id'],aws_secret_access_key=creds['aws_secret_access_key'])
awscreds = stscli.get_session_token(DurationSeconds=43200)
os.environ["AWS_ACCESS_KEY_ID"] = awscreds["Credentials"]["AccessKeyId"]
os.environ["AWS_SECRET_ACCESS_KEY"] = awscreds["Credentials"]["SecretAccessKey"]
os.environ["AWS_SESSION_TOKEN"] = awscreds["Credentials"]["SessionToken"]

bqd.copyMetadataFilesToS3(BQMETADATADESTINATION) ## expects the stat files generated by getTableStats() to be present in the CWD

bex = BQDataExtractor()
print("Extracting Raw Data")
result = bex.extractRawDataV3(BQRAWDATAFILESLOCATION)
#print("Mimic'ing BQ structure in GCS")
#result = bex.mimicBQStructureInGCS(BQMIMICEDDATABUCKET,BQMIMICEDDATAPREFIX)

spark = SparkSession.builder.appName("PySpark BQ Data Transporter").getOrCreate()
with open('BQTableDetails.json', 'r') as f:
    tabledet = json.load(f)

storage_client = storage.Client()
blobs = storage_client.list_blobs(
            GCSRAWDATABUCKET,prefix=GCSRAWDATAPREFIX + "raw/",delimiter=None
        )

## Tables that required Queries had to have data exported will be in GZIP compressed JSON formot.

for everyfile in blobs:
    datasetid,tableid = (everyfile.name).split('/')[2],(everyfile.name).split('/')[3]
    for tablet in tabledet:
        if datasetid == tablet["dataset_id"] and tableid == tablet["table_id"]:
            if tablet["is_partitioned"] == False:
                print("Staging {}".format(everyfile.name))
                avdf = spark.read.format("avro").load("gs://" + GCSRAWDATABUCKET + "/" + everyfile.name)
                destination_file = "gs://" + BQMIMICEDDATABUCKET + "/" + BQMIMICEDDATAPREFIX + '/'.join((everyfile.name).split('/')[2:-1])
                avdf.write.parquet(destination_file)
                print("Staged {}".format(everyfile.name))
                break
            if tablet["is_partitioned"] == True and tablet["partition_type"] == "TIME" and tablet["time_partition_type"] == "DAY" and tablet["time_partition_field"] == None:
                print("Staging {}".format(everyfile.name))
                avdf = spark.read.format("avro").load("gs://" + GCSRAWDATABUCKET + "/" + everyfile.name)
                destination_file = "gs://" + BQMIMICEDDATABUCKET + "/" + BQMIMICEDDATAPREFIX + '/'.join((everyfile.name).split('/')[2:-1])
                avdf.write.parquet(destination_file)
                print("Staged {}".format(everyfile.name))
                break
            if tablet["is_partitioned"] == True and tablet["partition_type"] == "TIME" and tablet["time_partition_type"] == "DAY" and tablet["time_partition_field"] != None:
                print("Staging {}".format(everyfile.name))
                avdf = spark.read.format("avro").load("gs://" + GCSRAWDATABUCKET + "/" + everyfile.name)
                destination_file = "gs://" + BQMIMICEDDATABUCKET + "/" + BQMIMICEDDATAPREFIX + '/'.join((everyfile.name).split('/')[2:-1])
                avdf.write.parquet(destination_file)
                print("Staged {}".format(everyfile.name))
                break
            if tablet["is_partitioned"] == True and tablet["partition_type"] == "TIME" and tablet["time_partition_type"] == "HOUR" and tablet["time_partition_field"] == None:
                print("Staging {}".format(everyfile.name))
                source_bucket = storage_client.bucket(GCSRAWDATABUCKET)
                source_blob = source_bucket.blob(everyfile.name)
                destination_bucket = storage_client.bucket(BQMIMICEDDATABUCKET)
                blob_copy = source_bucket.copy_blob(
                    source_blob, destination_bucket, BQMIMICEDDATAPREFIX + '/'.join((everyfile.name).split('/')[2:])
                )
                print("Staged {}".format(everyfile.name))
                break
            if tablet["is_partitioned"] == True and tablet["partition_type"] == "RANGE": ### WIP ###
                print("Staging {}".format(everyfile.name))
                source_bucket = storage_client.bucket(GCSRAWDATABUCKET)
                source_blob = source_bucket.blob(everyfile.name)
                destination_bucket = storage_client.bucket(BQMIMICEDDATABUCKET)
                blob_copy = source_bucket.copy_blob(
                    source_blob, destination_bucket, BQMIMICEDDATAPREFIX + '/'.join((everyfile.name).split('/')[2:])
                )
                print("Staged {}".format(everyfile.name))
                break
            
exit(0)

## WIP ##
#for tab in tabledet:
#    source_uri = BQRAWDATAFILESLOCATION + "raw/" + tab["dataset_id"] + "/" + everytable["table_id"] + "/"
#    if tab["is_partitioned"] == False:
#        source_uri = source_uri + "*"
#        ## Read all files into a dataframe
#        df = spark.read.format("avro").load(source_uri)
#        destination_uri = "gs://" + BQMIMICEDDATABUCKET + "/" + BQMIMICEDDATAPREFIX + tab["dataset_id"] + "/" + tab["table_id"] + "/"
#        df.write.parquet(destination_uri,mode="overwrite",compression="snappy")
    #if tab["is_partitioned"] == True and tab["partition_type"] == "TIME" and tab["time_partition_type"] == "DAY" and tab["time_partition_field"] == None: ## For ingestion time partitioned data
        #base_source_uri = source_uri + "*"
        #df = spark.read.format("avro").load(source_uri)
        #base_destination_uri = "gs://" + BQMIMICEDDATABUCKET + "/" + BQMIMICEDDATAPREFIX + tab["dataset_id"] + "/" + tab["table_id"] + "/"
        #df.write.parquet
